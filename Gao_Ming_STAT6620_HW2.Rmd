---
title: "Lazy Learning - Classification Using Nearst Neighbors"
output:
  pdf_document: default
  html_notebook: default
---



#Step 1 - collecting data


#Step 2 - exploring and preparing the data


Import local CSV file into R
```{r}
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
```

Drop the ID column - prevent overfitting
```{r}
wbcd <- wbcd[-1]
```

table of interest
```{r}
table(wbcd$diagnosis)
```

recode diagnosis as a factor and table or proportions with more informative labels

```{r}
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"),
                         labels = c("Benign", "Malignant"))

round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
```

Overview of three features
```{r}
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```

Create a normalize() function
```{r}
normalize <- function(x){return ((x-min(x)) / (max(x)-min(x)))}
```

test the function on a couple of vectors:
```{r}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
```


normalize the remaining 30 numeric features
```{r}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))
```

```{r}
summary(wbcd_n$area_mean)
```

Data preparation - creating training and test datasets
```{r}
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
```
```{r}
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

#Step 3 - training a model on the data

```{r}
library(class)
library(gmodels)
```


KNN() function - K Nearest Neighbor

```{r}
wbcd_test_pred <- knn(train = wbcd_train, test=wbcd_test, cl=wbcd_train_labels, k=21)
```

#Step 4 - evaluating model performance
```{r}
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
```

Accuracy of the prediction  - Min_Max Normalization Method K=21

The accuracy of the predication for min-max normalization and k=21 method produce a 98% accuracy. 


```{r}
(61+37) /100
```

#Step 5 - improving model performance

Predict outcome by using Z-score transformation

Note: Z-score transformation failed to improve the model. Therefore, the min-max normalization method with k =21 is used for knn prediction.


Transformation - Z-score standardization
```{r}
wbcd_z <- as.data.frame(scale(wbcd[-1]))
```


```{r}
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```



```{r}
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test,
cl = wbcd_train_labels, k = 21)

CrossTable(x = wbcd_test_labels, y = wbcd_test_pred,
prop.chisq = FALSE)
```

#start time
```{r}
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=1)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=5)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=11)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=15)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=21)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)

wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k=27)
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq=FALSE)
```




#Adult Example From UCI Repository

#Step 1 Collecting Data

#Step 2 Exploring and Preparing the Data

```{r}
library(RCurl)
urlfile <-'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
downloaded <- getURL(urlfile, ssl.verifypeer=FALSE)
connection <- textConnection(downloaded)
df <- read.csv(connection, header=FALSE, stringsAsFactors = FALSE)

str(df)
table(df$V5)
summary(df)

df <- df[sample(1:nrow(df)), ]

df$V5 <- factor(df$V5, levels = c("Iris-setosa", "Iris-virginica", "Iris-versicolor"))
round(prop.table(table(df$V5)) * 100, digits = 1)
str(df$V5)
```


```{r}
levels(df$V5)
normalize <- function(x){return ((x-min(x)) / (max(x)-min(x)))}
df_n <- as.data.frame(lapply(df[1:4], normalize))
summary(df_n)
```
```{r}
df_train <- df_n[1:100,]
df_test <- df_n[101:150,]
df_train_labels <- df[1:100, 5]
df_test_labels <- df[101:150, 5]
```



#Step 3 - training a model on the data

```{r}
library(class)
library(gmodels)
```


KNN() function - K Nearest Neighbor

```{r}
df_test_pred <- knn(train = df_train, test=df_test, cl=df_train_labels, k=11)
```

#Step 4 - evaluating model performance
```{r}
CrossTable(x = df_test_labels, y = df_test_pred, prop.chisq=FALSE)
```


#Step 5 - improving model performance

Adjust K value

```{r}
df_test_pred <- knn(train = df_train, test=df_test, cl=df_train_labels, k=9)
CrossTable(x = df_test_labels, y = df_test_pred, prop.chisq=FALSE)
```

In conclusion, the min-max normalization with k=9 is the optimal method for the prediction which produce a 100% accuracy based on a 50 test sample.




